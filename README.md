# San Francisco Crime

## Introduction

The motivation for this project stems from the San Francisco public database of police reports. The database consists of information on every logged police report ranging from 2003 to 2017. The information includes things like resolution, category, location, and description. I became interested in predicting the resolution of serious crimes in San Francisco. I defined serious crimes as must felony offenses and, in particular, violent crimes. The reduction of the original two million plus rows can be found in the "clean_data.py" file. Essentially, I decided to reduce the data down to, what I considered, serious crimes and crimes that were not resolved at the scene of the incident.

## Exploratory Data Analysis

I began my exploration of the data by creating a heatmap that takes locations of crimes and outputs them on the GoogleMaps API. This information can be seen in the html and python files named "heatmap." The two heatmaps I created are for serious crimes in the year 2017. From this visualization it was easily attained that the majority of crimes occurred in the same areas. Furthermore, I found that roughly sixty percent of all crimes go unsolved. Once I had this information I decided to focus on crimes that I found to be more intriguing. In particular, these were crimes that made me feel "unsafe" in a manner of speaking. They were the crimes that I believed to be the most detrimental to a functioning society. To my surprise, after reducing the data, I found that these crimes were actually less likely to have a resolution, something I found to be extremely worrisome.

## Base Model

To get meaningful results I created a base model from a significantly reduced dataset. I essentially removed all text columns and ran Logistic Regression on the remaining information. In particular, the most significant remaining information pertained to the location. The Accuracy, Precision, and Recall from this model were 62.5%, 62%, and .063% respectively. This model was essentially predicting the majority class for each data point, producing results that mirrored the proportions of resolved to non-resolved in the full dataset.

## Improved Base Model

To improve the results above I created dummy variables out of the remaining categorical information. This resulted in "dummifiying" pddistrict, category (of crime) and day of the week. In addition, I expanded the timestamp column into year, month, day and hour that the crime occurred. Overall this improved the results of the model. In a Random Forest Classifier this resulted in an accuracy, precision, and recall of 82%, 77%, and 75%, respectively. The code for this transformation can be found in "clean_data.py". The code for running the models can be found in "models.py".

## The Final Model

Although the changes stated above improved the accuracy by nearly 33%, I decided to take it one step further. To improve the results I decided to analyze the description column. I took a two path approach. Each approach involved applying TF-IDF to the description column. I took the results and applied a Random Forest Classifier only on this in respect to the resolution. Not surprisingly, there was a significant amount of signal. From this I decided to reduce the resulting matrix into only the important features. After cross validating how many features resulted in the most improvement I found that adding features past the top 100 features (out of the 700+ features) didn't improve results. Upon doing this I found that the model was taking significantly more time to fit and predict. To reduce this time I decided to apply Non-negative matrix factorization to the Tf-Idf matrix. In a similar fashion, I cross-validated how many latent features to add to the full dataframe. Similarly to the individual word features, I found that concatenating more than 50 latent features did not improve results. Since the 50 latent features had similar results to the 100 top features from the Tf-Idf, I decided to apply the fifty latent features method. This was in the interest of creating a model that was faster to fit and quicker to predict. My final results were accuracy: 87%, precision: 86%, and recall: 76%.

## Conclusion
In conclusion I found that my best model was a product of dummy variable and natural language processing. I also found my results would have been improved from including crimes that I considered less serious. In addition, removing crimes that were solved at the scene of the incident significantly reduced my scores. However, I was able to increase the reduced scores to a seemingly acceptable level. I attribute the remaining delta to the noise associated with these particular crimes. It is also not surprising that the crimes that are not immediately solved are the ones that are more difficult to predict the resolution for. The fact that location and pddistrict are the most important features in my model also attests to the commonly accepted characteristic of criminal behavior in the city of San Francisco. For next steps, I would like to create a few more predictors. Specifically, I would like to create a predictor by pddistrict, create a predictor for the type of crime given location, a number of predictors for other categories of crimes (less serious/non-felony), and (more abstractly) create a predictor for the probability of experiencing a crime in any given location.
