
model = RandomForestClassifier()
#
y = basic_df.pop('resolution').values
x_feat = tf.values
X_train_feat, X_test_feat, y_train_feat, y_test_feat = train_test_split(x_feat,y, random_state=42)
model.fit(X_train_feat,y_train_feat)
importances = {}
scores = {}
importance_df = pd.DataFrame({'feature':tf.columns,'importance':np.round(model.feature_importances_,5)})
importance_df = importance_df.sort('importance', ascending=False)
# #
for i in xrange(1,500,50):
    df = dc.add_vectorized_matrix_to_df(tf[top_imps.feature[:i].values.tolist()])
    x = df.values
    X_train, X_test, y_train, y_test = train_test_split(x,y, random_state=42)
    model.fit(X_train,y_train)
    prediction = model.predict(X_test)
    print i, 'features'
    print 'accuracy:', accuracy_score(y_test,prediction)
    print 'precision:', precision_score(y_test,prediction)
    print 'recall:', recall_score(y_test,prediction)
    print '\n\n'
    importances[i] = pd.DataFrame({'feature':df.columns,'importance':np.round(model.feature_importances_,5)})
    scores[i] =  [accuracy_score(y_test,prediction),precision_score(y_test,prediction),recall_score(y_test,prediction)]




files = ['importances1.csv','importances2.csv','importances3.csv','importances4.csv','importances5.csv','importances6.csv','importances7.csv','importances8.csv','importances9.csv','importances10.csv']
for i, f in zip(importances.keys(),files):
     importances[i].to_csv(f,index=False)


files = ['importances1.csv','importances2.csv','importances3.csv','importances4.csv','importances5.csv','importances6.csv','importances7.csv','importances8.csv','importances9.csv','importances10.csv']
importances_dictionary = {}
for name,path in zip([name.replace('.csv','') for name in files],files):
    importances_dictionary[name]= pd.read_csv(path)

for key,value in  importances_dictionary.iteritems():
    importances_dictionary[key] = value.sort_values('importance',ascending=False)

fig, axes = plt.subplots(5,2)
for ax, (key, value) in zip(axes.ravel(),importances_dictionary.iteritems()):
    labels = value.head(19).feature
    ax.set_xticklabels(xrange(1,20), rotation=90 )
    ax.bar(xrange(1,20),value.importance.head(19),tick_label = labels,align='center')

    # scores = {}
    # all_topics = {}
    # for i in [1,3,5,7,10]:
    #     w,topics = dc.reduce_with_NMF(tf,features,i*10)
    #     w = pd.DataFrame(w)
    #     df = dc.add_vectorized_matrix_to_df(w)
    #     x = dc.df.values
    #     X_train, X_test, y_train, y_test = train_test_split(x,y, random_state=42)
    #     model.fit(X_train,y_train)
    #     prediction = model.predict(X_test)
    #     print 'accuracy:', accuracy_score(y_test,prediction)
    #     print 'precision:', precision_score(y_test,prediction)
    #     print 'recall:', recall_score(y_test,prediction)
    #     print '\n\n'
    #     scores[i] = [accuracy_score(y_test,prediction),precision_score(y_test,prediction),recall_score(y_test,prediction)]
    #     all_topics[i] = topics
gets first value for each score (accuracy)

scores =  collections.OrderedDict(sorted(scores.items()))

for i in xrange(3):
  plt.plot(scores.keys(),[item[i] for item in scores.values()])


  plt.legend(['Accuracy', 'Precision', 'Recall'], loc='upper right')


  # run models.py
  # m = Models('/Users/hercules/Desktop/Galvanize/capstone/data/vectorize_test.csv','resolution')
  # m.fit_models()
  # m.model_scores()
  # m.plot_top_feature_importances(10,True)



      # def make_x_y(self, self.y_column):
      #     y = self.df.pop(y_column).values
      #     if logreg:
      #         self.df = self.drop_columns(dummy_columns_to_drop)
      #         x = self.df.values
      #     else:
      #         x = self.df.values
      #     return x,y


      # def train_test(self,x,y):
      #     return train_test_split(x,y)



      # gettint the best number of topics
  # scores = {}
  # all_topics = {}
  # for i in [1,3,5,7,10]:
  #     w,topics = dc.reduce_with_NMF(tf,features,i*10)
      # w = pd.DataFrame(w)
      # df = dc.add_vectorized_matrix_to_df(w)
      # x = dc.df.values
      # X_train, X_test, y_train, y_test = train_test_split(x,y, random_state=42)
      # model.fit(X_train,y_train)
      # prediction = model.predict(X_test)
      # print 'accuracy:', accuracy_score(y_test,prediction)
      # print 'precision:', precision_score(y_test,prediction)
      # print 'recall:', recall_score(y_test,prediction)
      # print '\n\n'
      # scores[i] = [accuracy_score(y_test,prediction),precision_score(y_test,prediction),recall_score(y_test,prediction)]
      # all_topics[i] = topics



      #WORK FLOW FOR GENERAL DATAFRAME
      # resolutions_to_delete = ['UNFOUNDED','JUVENILE CITED','JUVENILE ADMONISHED','JUVENILE DIVERTED','CLEARED-CONTACT JUVENILE FOR MORE INFO','PROSECUTED FOR LESSER OFFENSE']
      # dc.delete_rowvalues('resolution',resolutions_to_delete)

      here is why i got rid of these

      can you add the feature importance/how to get aggregated feature importance

      decide which to use NMF or features
      #     # -- Other values to delete:
      # categories_to_delete =  ['OTHER OFFENSES','VANDALISM','SUSPICIOUS OCC','RUNAWAY','SUICIDE','FAMILY OFFENSES','NON-CRIMINAL','FRAUD','SECONDARY CODES', 'FORGERY/COUNTERFEITING','TRESPASS','DISORDERLY CONDUCT','DRUNKENNESS','DRIVING UNDER THE INFLUENCE','LIQUOR LAWS','LOITERING','BAD CHECKS','SEX OFFENSES, NON FORCIBLE','GAMBLING','PORNOGRAPHY/OBSCENE MAT','TREA']
      # dc.delete_rowvalues('category',categories_to_delete)
      #
      # dc.make_binary('resolution', ['NONE'])
      # # write down which numbers coorespond to which
      # for column in ['pddistrict','category','dayofweek']:
      #     dc.create_dummies(column)
      # # dc.replace_all_categoricals(['pddistrict','category','dayofweek'])
      # #     -- cant do this. NEED TO MAKE DUMMIES OUT OF THESE
      #
      # dc.drop_columns(['pdid','unique_key','address','location'])
      #
      # for date in ['year','month','day','hour']:
      #     dc.time_column(date)
      # dc.drop_columns('timestamp')
      # features,tf = dc.vectorize('descript')
      # w,topics = dc.reduce_with_NMF(tf,features,50)
      # w = pd.DataFrame(w)
      # df = dc.add_vectorized_matrix_to_df(w)
      #
